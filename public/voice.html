<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sommalier Gent - Voice Agent</title>
    <link rel="stylesheet" href="styles.css">
    <style>
        .voice-controls {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100%;
            gap: 30px;
        }

        .voice-button {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            border: none;
            background: rgba(255, 255, 255, 0.2);
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
            cursor: pointer;
            transition: all 0.3s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 48px;
            color: white;
            box-shadow: 0 4px 16px rgba(0, 0, 0, 0.3);
        }

        .voice-button:hover {
            background: rgba(255, 255, 255, 0.3);
            transform: scale(1.05);
        }

        .voice-button.listening {
            background: rgba(255, 0, 0, 0.5);
            animation: pulse 1.5s infinite;
        }

        .voice-button.speaking {
            background: rgba(0, 255, 0, 0.5);
            animation: pulse 1.5s infinite;
        }

        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.1); }
        }

        .status-text {
            color: white;
            font-size: 18px;
            text-align: center;
            min-height: 24px;
        }

        .conversation-log {
            width: 100%;
            max-height: 400px;
            overflow-y: auto;
            padding: 20px;
            background: rgba(255, 255, 255, 0.05);
            border-radius: 15px;
            margin-bottom: 20px;
            display: none;
        }

        .conversation-log.show {
            display: block;
        }

        .message {
            margin-bottom: 15px;
            padding: 10px 15px;
            border-radius: 10px;
            max-width: 80%;
        }

        .message.user {
            background: rgba(255, 255, 255, 0.2);
            margin-left: auto;
            text-align: right;
        }

        .message.assistant {
            background: rgba(255, 255, 255, 0.1);
            margin-right: auto;
        }

        .toggle-log {
            position: absolute;
            top: 20px;
            right: 20px;
            padding: 10px 20px;
            background: rgba(255, 255, 255, 0.2);
            border: 1px solid rgba(255, 255, 255, 0.3);
            border-radius: 20px;
            color: white;
            cursor: pointer;
            font-size: 14px;
        }
    </style>
</head>
<body>
    <div class="background-container">
        <img src="background.png" alt="Background" class="background-image">
    </div>
    
    <div class="modal-container" id="agentModal">
        <div class="modal-content">
            <button class="toggle-log" onclick="toggleConversationLog()">Toggle Log</button>
            
            <div class="conversation-log" id="conversationLog"></div>
            
            <div class="voice-controls">
                <button class="voice-button" id="voiceButton" onclick="toggleVoiceInteraction()">
                    üé§
                </button>
                <div class="status-text" id="statusText">Click the microphone to start</div>
            </div>
        </div>
    </div>

    <script>
        let isListening = false;
        let recognition = null;
        let conversationHistory = [];
        let isProcessing = false;

        // Initialize speech recognition
        function initSpeechRecognition() {
            if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                recognition = new SpeechRecognition();
                recognition.continuous = false;
                recognition.interimResults = false;
                recognition.lang = 'en-US';

                recognition.onstart = function() {
                    isListening = true;
                    updateUI('listening');
                    document.getElementById('statusText').textContent = 'Listening...';
                };

                recognition.onresult = function(event) {
                    const transcript = event.results[0][0].transcript;
                    addMessage('user', transcript);
                    sendToOpenAI(transcript);
                };

                recognition.onerror = function(event) {
                    console.error('Speech recognition error:', event.error);
                    updateUI('idle');
                    document.getElementById('statusText').textContent = 'Error: ' + event.error;
                    isListening = false;
                };

                recognition.onend = function() {
                    isListening = false;
                    if (!isProcessing) {
                        updateUI('idle');
                        document.getElementById('statusText').textContent = 'Click the microphone to start';
                    }
                };
            } else {
                alert('Speech recognition is not supported in your browser. Please use Chrome or Edge.');
            }
        }

        function toggleVoiceInteraction() {
            if (isProcessing) {
                return;
            }

            if (!recognition) {
                initSpeechRecognition();
                return;
            }

            if (isListening) {
                recognition.stop();
                updateUI('idle');
                document.getElementById('statusText').textContent = 'Stopped listening';
            } else {
                try {
                    recognition.start();
                } catch (err) {
                    console.error('Error starting recognition:', err);
                    document.getElementById('statusText').textContent = 'Error starting recognition';
                }
            }
        }

        async function sendToOpenAI(userMessage) {
            isProcessing = true;
            updateUI('processing');
            document.getElementById('statusText').textContent = 'Processing...';

            try {
                const response = await fetch('/api/voice-chat', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        message: userMessage,
                        history: conversationHistory
                    })
                });

                if (!response.ok) {
                    throw new Error('API request failed');
                }

                const data = await response.json();
                const assistantMessage = data.response;
                
                conversationHistory.push({ role: 'user', content: userMessage });
                conversationHistory.push({ role: 'assistant', content: assistantMessage });
                
                addMessage('assistant', assistantMessage);
                speakText(assistantMessage);
            } catch (error) {
                console.error('Error:', error);
                document.getElementById('statusText').textContent = 'Error: ' + error.message;
                updateUI('idle');
            } finally {
                isProcessing = false;
            }
        }

        function speakText(text) {
            if ('speechSynthesis' in window) {
                updateUI('speaking');
                document.getElementById('statusText').textContent = 'Speaking...';
                
                const utterance = new SpeechSynthesisUtterance(text);
                utterance.lang = 'en-US';
                utterance.rate = 1.0;
                utterance.pitch = 1.0;
                utterance.volume = 1.0;

                utterance.onend = function() {
                    updateUI('idle');
                    document.getElementById('statusText').textContent = 'Click the microphone to continue';
                };

                utterance.onerror = function(event) {
                    console.error('Speech synthesis error:', event);
                    updateUI('idle');
                    document.getElementById('statusText').textContent = 'Error speaking';
                };

                window.speechSynthesis.speak(utterance);
            } else {
                alert('Speech synthesis is not supported in your browser.');
            }
        }

        function updateUI(state) {
            const button = document.getElementById('voiceButton');
            button.className = 'voice-button';
            
            if (state === 'listening') {
                button.classList.add('listening');
                button.textContent = 'üî¥';
            } else if (state === 'speaking') {
                button.classList.add('speaking');
                button.textContent = 'üîä';
            } else if (state === 'processing') {
                button.textContent = '‚è≥';
            } else {
                button.textContent = 'üé§';
            }
        }

        function addMessage(role, text) {
            const log = document.getElementById('conversationLog');
            log.classList.add('show');
            
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${role}`;
            messageDiv.textContent = text;
            log.appendChild(messageDiv);
            log.scrollTop = log.scrollHeight;
        }

        function toggleConversationLog() {
            const log = document.getElementById('conversationLog');
            log.classList.toggle('show');
        }

        // Initialize on page load
        window.addEventListener('load', function() {
            initSpeechRecognition();
        });
    </script>
</body>
</html>

